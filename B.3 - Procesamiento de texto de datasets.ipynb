{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso los textos en datasetunificado, para ver si se pueden mejorar las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#0 Crowdflower, 1 Electoral Tweets, 2 EmoInt, 3 TEC\n",
    "def openTemp(lang, file):\n",
    "    file_names = [\"datasetCrowdflower2-\", \"datasetElectoral2-\", \"datasetEmoInt2-\", \"datasetTec2-\"]\n",
    "    \n",
    "    path_datasets = 'datasets/united/'\n",
    "    pathfile = path_datasets+lang+'/'+file_names[file]+lang+'.csv'\n",
    "    \n",
    "    df = pd.read_csv(pathfile,'r',delimiter='\\t',encoding='UTF-16').dropna().reset_index(drop=True)\n",
    "    df['emotion'] = df['emotion'].astype('int')\n",
    "    df['text'] = df['text'].astype('U')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Dado un idioma, devuelvo un dataframe con el dataset sin procesar de ese idioma\n",
    "def abrirArchivo(lang):\n",
    "    path_datasets = 'datasets/united/'\n",
    "    pathfile = path_datasets+lang+'/datasetUnificado2-'+lang+'.csv'\n",
    "    \n",
    "    df = pd.read_csv(pathfile,'r',delimiter='\\t',encoding='UTF-16').dropna().reset_index(drop=True)\n",
    "    df['emotion'] = df['emotion'].astype('int')\n",
    "    df['text'] = df['text'].astype('U')\n",
    "    print(pathfile)\n",
    "    print(\"after abrirArchivo(): \" + str(df.count()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino funciones para eliminar los elementos innecesarios de los textos\n",
    "import regex as re\n",
    "\n",
    "#Dado un DataFrame con una lista de frases, filtro sus hashtags\n",
    "def remove_hashtags(df_sentences):\n",
    "    for i in range(len(df_sentences)):\n",
    "        df_sentences.loc[i, ('text')] = re.sub(r'#(\\s)?(\\w)+|(#)', '', df_sentences.loc[i, ('text')])\n",
    "\n",
    "#Dado un DataFrame con una lista de frases, filtro \n",
    " #el RT @Usuario al inicio del campo texto de los retweets y los @Usuario en cualquier parte del texto\n",
    "def remove_rt(df_sentences):\n",
    "    for i in range(len(df_sentences)):\n",
    "        df_sentences.loc[i, ('text')] = re.sub(r'(rt @\\w+(:\\s)?)|(@\\w+)|(@)|(RT @\\w+(:\\s)?)', '', df_sentences.loc[i, ('text')])\n",
    "\n",
    "#Dado un DataFrame con una lista de frases, filtro las URLs\n",
    "def remove_url(df_sentences):\n",
    "    for i in range(len(df_sentences)):\n",
    "        df_sentences.loc[i, ('text')] = re.sub(r'(http(s)?://.*)+', '', df_sentences.loc[i, ('text')])\n",
    "\n",
    "#Dado un DataFrame con una lista de frases, filtro los signos de puntuacion\n",
    "def remove_punctuation(df_sentences):\n",
    "    for i in range(len(df_sentences)):\n",
    "        df_sentences.loc[i, ('text')] = re.sub(r'[\\\\\\.,;:\\-_\\“\\”\\\"\\‘\\’\\'\\s$\\)\\(\\%]|(\\'s)', ' ', df_sentences.loc[i, ('text')])\n",
    "        df_sentences.loc[i, ('text')] = re.sub(r'(\\s*\\n\\s*)+|(\\s)+', ' ', df_sentences.loc[i, ('text')])\n",
    "\n",
    "#Dado un DataFrame con una lista de frases, llamo a todos los filtros definidos anteriormente \n",
    "def remove_all(df_sentences):\n",
    "    print(\"Before remove_all(): \" + str(len(df_sentences)))\n",
    "    remove_rt(df_sentences)\n",
    "    remove_hashtags(df_sentences)\n",
    "    remove_url(df_sentences)\n",
    "    remove_punctuation(df_sentences)\n",
    "    print(\"After remove_all(): \" + str(len(df_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quito las stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Dado un DataFrame con una lista de frases, filtro las stopwords\n",
    "def remove_stopwords(df_sentences, lang):\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    filtered_words = []\n",
    "    \n",
    "    for i in range(len(df_sentences)):\n",
    "        words_wo_stop_words = list()\n",
    "        words = df_sentences.loc[i, ('text')].split()\n",
    "        for word in words:\n",
    "            if(word.lower() not in stop_words):\n",
    "                words_wo_stop_words.append(word)\n",
    "        df_sentences.loc[i, ('text')] = ' '.join(words_wo_stop_words)\n",
    "    print(\"After remove_stopwords(): \" + str(len(df_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import lemminflect\n",
    "\n",
    "#Dado un DataFrame con una lista de frases y un modelo de spacy, paso palabras a sus lemmas\n",
    "def transform_lemmas(df_sentences, spacy_model):\n",
    "    nlp = spacy.load(spacy_model)\n",
    "\n",
    "    for i in range(len(df_sentences)):\n",
    "        words_lemmas = list()\n",
    "        words_nlp = nlp(df_sentences.loc[i, ('text')])\n",
    "        for word in words_nlp:\n",
    "            words_lemmas.append(word._.lemma())\n",
    "        df_sentences.loc[i, ('text')] = ' '.join(words_lemmas)\n",
    "    print(\"After transform_lemmas(): \" + str(len(df_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dado un DataFrame con una lista de frases, remuevo las frases vacias\n",
    "def remove_empty(df_tweets):\n",
    "    df_tweets = df_tweets.replace(r'( )+',' ', regex=True)\n",
    "    df_tweets = df_tweets.replace(r'(\\n)+','\\n', regex=True)\n",
    "    df_tweets = df_tweets.dropna(how='all')\n",
    "    \n",
    "    print(\"After remove_empty(): \" + str(len(df_tweets)))\n",
    "    return df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dado un DataFrame con una lista de frases, remuevo las frases duplicadas\n",
    "def remove_duplicates(df_tweets):\n",
    "    cuac =  df_tweets.drop_duplicates(subset=['text'])\n",
    "    print(\"After remove_duplicates(): \" + str(len(cuac)))\n",
    "    return cuac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recibo una lista de frases y datos sobre su procedencia y las escribo en un archivo\n",
    "def escribirArchivoDatasetProc(df, lang):\n",
    "    path_united = 'datasets/united/'+lang+'/'\n",
    "    filename = 'datasetUnificadoProcesado2-'+lang+'.csv'\n",
    "    filepath = path_united + filename\n",
    "    df.to_csv(filepath, encoding='utf-16', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recibo una lista de frases y datos sobre su procedencia y las escribo en un archivo\n",
    "def escribirArchivoDatasetProcTemp(df, lang,file):\n",
    "    file_names = [\"datasetCrowdflower\", \"datasetElectoral\", \"datasetEmoInt\", \"datasetTec\"]\n",
    "    \n",
    "    filename = file_names[file] + 'Procesado2-'+lang+'.csv'\n",
    "    path_united = 'datasets/united/'+lang+'/'\n",
    "    filepath = path_united + filename\n",
    "    df.to_csv(filepath, encoding='utf-16', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before remove_all(): 39727\n",
      "After remove_all(): 39727\n",
      "After remove_stopwords(): 39727\n",
      "After transform_lemmas(): 39727\n",
      "After remove_empty(): 39727\n",
      "After remove_duplicates(): 38062\n",
      "Before remove_all(): 4055\n",
      "After remove_all(): 4055\n",
      "After remove_stopwords(): 4055\n",
      "After transform_lemmas(): 4055\n",
      "After remove_empty(): 4055\n",
      "After remove_duplicates(): 1251\n",
      "Before remove_all(): 7097\n",
      "After remove_all(): 7097\n",
      "After remove_stopwords(): 7097\n",
      "After transform_lemmas(): 7097\n",
      "After remove_empty(): 7097\n",
      "After remove_duplicates(): 6084\n",
      "Before remove_all(): 13249\n",
      "After remove_all(): 13249\n",
      "After remove_stopwords(): 13249\n",
      "After transform_lemmas(): 13249\n",
      "After remove_empty(): 13249\n",
      "After remove_duplicates(): 13094\n",
      "Before remove_all(): 39730\n",
      "After remove_all(): 39730\n",
      "After remove_stopwords(): 39730\n",
      "After transform_lemmas(): 39730\n",
      "After remove_empty(): 39730\n",
      "After remove_duplicates(): 38312\n",
      "Before remove_all(): 4055\n",
      "After remove_all(): 4055\n",
      "After remove_stopwords(): 4055\n",
      "After transform_lemmas(): 4055\n",
      "After remove_empty(): 4055\n",
      "After remove_duplicates(): 1269\n",
      "Before remove_all(): 7097\n",
      "After remove_all(): 7097\n",
      "After remove_stopwords(): 7097\n",
      "After transform_lemmas(): 7097\n",
      "After remove_empty(): 7097\n",
      "After remove_duplicates(): 5994\n",
      "Before remove_all(): 13251\n",
      "After remove_all(): 13251\n",
      "After remove_stopwords(): 13251\n",
      "After transform_lemmas(): 13251\n",
      "After remove_empty(): 13251\n",
      "After remove_duplicates(): 13102\n"
     ]
    }
   ],
   "source": [
    "#bloque temporal de procesamiento de archivos\n",
    "#es para procewsar individualmente crowflowerrt, electora, emoint y tec\n",
    "dic_stopwords_lang = {'es':'spanish', 'en':'english', 'pt':'portuguese'}\n",
    "dic_spacy_lang = {'es':'es_core_news_lg', 'en':'en_core_web_lg', 'pt':'pt_core_news_lg'}\n",
    "\n",
    "for lang in ['en','es','pt']:\n",
    "    stopwords_lang = dic_stopwords_lang[lang]\n",
    "    spacy_lang = dic_spacy_lang[lang]\n",
    "    for file in [0,1,2,3]:\n",
    "        df = openTemp(lang,file)\n",
    "        remove_all(df)\n",
    "        remove_stopwords(df, stopwords_lang)\n",
    "        transform_lemmas(df, spacy_lang)\n",
    "        df = remove_empty(df)\n",
    "        df = remove_duplicates(df)\n",
    "        escribirArchivoDatasetProcTemp(df, lang, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/united/pt/datasetUnificado2-pt.csv\n",
      "after abrirArchivo(): text       64133\n",
      "emotion    64133\n",
      "dtype: int64\n",
      "Before remove_all(): 64133\n",
      "After remove_all(): 64133\n",
      "After remove_stopwords(): 64133\n",
      "After transform_lemmas(): 64133\n",
      "After remove_empty(): 64133\n",
      "After remove_duplicates(): 58662\n",
      "datasets/united/es/datasetUnificado2-es.csv\n",
      "after abrirArchivo(): text       64128\n",
      "emotion    64128\n",
      "dtype: int64\n",
      "Before remove_all(): 64128\n",
      "After remove_all(): 64128\n",
      "After remove_stopwords(): 64128\n",
      "After transform_lemmas(): 64128\n",
      "After remove_empty(): 64128\n",
      "After remove_duplicates(): 58457\n"
     ]
    }
   ],
   "source": [
    "dic_stopwords_lang = {'es':'spanish', 'en':'english', 'pt':'portuguese'}\n",
    "dic_spacy_lang = {'es': 'es_core_news_lg', 'en':'en_core_web_lg', 'pt':'pt_core_news_lg'}\n",
    "\n",
    "for lang in ['pt','es', 'en']:\n",
    "    stopwords_lang = dic_stopwords_lang[lang]\n",
    "    spacy_lang = dic_spacy_lang[lang]\n",
    "    df = abrirArchivo(lang)\n",
    "    remove_all(df)\n",
    "    remove_stopwords(df, stopwords_lang)\n",
    "    transform_lemmas(df, spacy_lang)\n",
    "    df = remove_empty(df)\n",
    "    df = remove_duplicates(df)\n",
    "    escribirArchivoDatasetProc(df, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EN:\n",
    "\n",
    "+ Original: 64147\n",
    "+ remove_duplicates: 60833\n",
    "+ remove_all + remove_dup: 58098\n",
    "+ remove _all - remove_punct + remove_dup: 59173\n",
    "\n",
    "#### ES:\n",
    "+ Original: 64126\n",
    "+ remove_all + remove_dup: \n",
    "\n",
    "#### PT:\n",
    "+ Original: 64133\n",
    "+ remove_all + remove_dup: 58622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino esta funcion para hacer un archivo aparte con todo el procesamiento del texto mas los indices resultantes\n",
    "def escribirArchivoAnalisis(df, lang):\n",
    "    path_united = 'datasets/analisis/'\n",
    "    filename = 'datasetConIndice-'+lang+'.csv'\n",
    "    filepath = path_united + filename\n",
    "    df.to_csv(filepath, encoding='utf-16', index=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/united/es/datasetUnificado2-es.csv\n",
      "after abrirArchivo(): text       64126\n",
      "emotion    64126\n",
      "dtype: int64\n",
      "datasets/united/pt/datasetUnificado2-pt.csv\n",
      "after abrirArchivo(): text       64133\n",
      "emotion    64133\n",
      "dtype: int64\n",
      "datasets/united/en/datasetUnificado2-en.csv\n",
      "after abrirArchivo(): text       64147\n",
      "emotion    64147\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_es = abrirArchivo('es')\n",
    "df_pt = abrirArchivo('pt')\n",
    "df_en = abrirArchivo('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c9a60fd698f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64142</th>\n",
       "      <td>About to have a movie night with my booboo @je...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64143</th>\n",
       "      <td>@TheBodyShopUK Knowing my dissertation will be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64144</th>\n",
       "      <td>hospital tomorrow morning strapped with wires ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64145</th>\n",
       "      <td>Work is soooo slow ready to have a great saturday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64146</th>\n",
       "      <td>You realize that by choosing joy every single ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  emotion\n",
       "64142  About to have a movie night with my booboo @je...        3\n",
       "64143  @TheBodyShopUK Knowing my dissertation will be...        1\n",
       "64144  hospital tomorrow morning strapped with wires ...        1\n",
       "64145  Work is soooo slow ready to have a great saturday        1\n",
       "64146  You realize that by choosing joy every single ...        1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64121</th>\n",
       "      <td>A punto de tener una noche de cine con mi boob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64122</th>\n",
       "      <td>@TheBodyShopUK Sabiendo que mi disertación est...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64123</th>\n",
       "      <td>hospital mañana por la mañana atado con cables...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64124</th>\n",
       "      <td>El trabajo es taaaaan lento, listo para tener ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64125</th>\n",
       "      <td>Te das cuenta de que eligiendo la alegría en c...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  emotion\n",
       "64121  A punto de tener una noche de cine con mi boob...        1\n",
       "64122  @TheBodyShopUK Sabiendo que mi disertación est...        3\n",
       "64123  hospital mañana por la mañana atado con cables...        7\n",
       "64124  El trabajo es taaaaan lento, listo para tener ...        4\n",
       "64125  Te das cuenta de que eligiendo la alegría en c...        7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_es.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64128</th>\n",
       "      <td>Prestes a ter uma noite de cinema com meu boob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64129</th>\n",
       "      <td>@TheBodyShopUK Sabendo que minha dissertação s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64130</th>\n",
       "      <td>hospital amanhã de manhã amarrado com arames e...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64131</th>\n",
       "      <td>O trabalho é muito lento, pronto para ter um ó...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64132</th>\n",
       "      <td>Você percebe que ao escolher a alegria em cada...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  emotion\n",
       "64128  Prestes a ter uma noite de cinema com meu boob...        1\n",
       "64129  @TheBodyShopUK Sabendo que minha dissertação s...        3\n",
       "64130  hospital amanhã de manhã amarrado com arames e...        7\n",
       "64131  O trabalho é muito lento, pronto para ter um ó...        4\n",
       "64132  Você percebe que ao escolher a alegria em cada...        7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pt.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
